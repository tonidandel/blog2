---
layout: post
title: p-Hacking - como a ciência de dados pode ser usada para provar qualquer coisa
categories: [data science, tecnologia]
---

É possível manipular um experimento para obter um resultado estatisticamente significativo? Aqui vou mostrar como se pode usar a ciência de dados para tirar qualquer conclusão a partir de um conjunto de dados. 

Antes que os conspiradores de plantão levantem uma lista do por que a indústria farmacêutica é capaz de descobrir, a cada momento, uma aplicação nova para a aspirina, ou finalmente decidir se o café faz bem ou mal, posso ir adiantando que a história tem sua contraparte verdadeira. Sim, é possível usar a estatística para tirar qualquer conclusão a partir de quaisquer conjuntos de dados. De certa forma, em como aquele advogado que pode conseguir convencer um juri da inexpugnabilidade do seu caso, caso ele tenha uma boa retórica. Isto é, como uma boa argumentação, é difícil perder um caso.

![teste-t](https://otelegrafo.com/images/p-value.png)

Encontrei recentemente [uma matéria interessante da revista Nature](https://www.nature.com/articles/nature.2017.22375)([^1]), sobre a questão do "valor-p", que é um conceito bastante conhecido em *Data Science*. A ciência de dados é uma ciência eminentemente estatística e, com o auxílio da computação que propicia o trabalho com grandes volumes de dados, tem propiciado uma verdadeira revolução nesta era dos algoritmos de Inteligência Artificial. Mas voltemos ao famigerado valor-p.

Basicamente o valor-p é um parâmetro estatístico que indica se determinada hipótese, formulada a priori, deve ser rejeitada ou não. Por exemplo, caso se deseje testar se `estudantes melhoram o desempenho acadêmico após tomarem uma xícara de café`, o problema pode ser formulado de acordo com as duas hipóteses $H_0$ e $H_1$, chamadas respectivamente de hipótese nula e alternativa:

    H0 - Hipótese "nula" : o "café não melhora o desempenho;

Contra

    H1 - A hipótese "alternativa" : indica que o "desempenho dos alunos melhora após uma xícara de café".

As duas hipóteses podem ser testadas a partir de determinados testes estatísticos, como o famoso [teste t de Student](https://pt.wikipedia.org/wiki/Teste_t_de_Student), que gera, como um dos parâmetros de decisão, o chamado "valor-p".

Se o valor-p for pequeno (geralmente menor que 5%), considera-se que há uma pequena chance de a explicação o `café melhora o desempenho dos alunos` seja devida ao acaso. Isto é, se $p<5\%$, pode-se rejeitar a hipótese nula (para um nível de significância de 5%) e adotar a hipótese alternativa, que indica que o café melhora o desempenho dos estudantes.

Se o valor p for grande (i.e., $p>0,05$) o resultado é inconclusivo. Até aí tudo bem, certo? Alguém consegue ver algum problema? 

Bem, o problema com certeza não está no resultado do teste em si, mas reside na crença quase absoluta de que o conceito do p-value -- que pertence a uma tradição de pensamento na estatística chamada de "Frequentista" -- é correto para a tomada de decisões, como atestar se determinada descoberta, como no exemplo do café, é verdadeira. 

Além disso, em um teste estatístico, como num [teste randomizado](https://pt.wikipedia.org/wiki/Estudo_cl%C3%ADnico_randomizado_controlado) -- muito comum em testes com seres humanos -- considera-se que as hipóteses são sempre estabelecidas antes da coleta dos dados, isto é, parte-se de uma premissa de que os dados são coletados "às cegas", o que nem sempre acontece. Por exemplo, se pretendemos comprovar a eficácia de uma nova droga no tratamento de uma doença, escolhe-se um grupo de pessoas que vão tomar o medicamento e outro grupo, chamado grupo de controle, que tomará apenas uma pílula de açucar, chamada de *placebo*. Em testes desse tipo, nem os aplicadores dos medicamentos aos grupos sabem quem faz parte de qual grupo, e, em muitas ocasiões, não tem a menor ideia de que se está realizando uma pesquisa desse tipo. Após a coleta, seguem-se os testes estatíticos que geram parâmetros como o valor-p.

O caso da crença no valor-p e, mais ainda, no intervalo de $5\%$ para o que é chamado "nível de significância" de uma descoberta estatística,  é exatamente o que o artigo da Nature trata, e o que comento aqui. Alguns cientistas tem razões para acreditar que há um equívoco na interpretação e uso indiscriminado do conceito, o que pode levar, inclusive, a uma taxa maior de "falsas descobertas", supondo uma pesquisa idônea.

O segundo problema, este mais grave por ser altamente anti-ético, está relacionado ao "p-hacking", que consiste basicamente em "ajustar" os dados de forma a se poder tirar qualquer conclusão que se queira a partir dos mesmos, que vem dodaí o termo *hacking*  (hackear, romper, burlar...), isto é, "maquiar" um conjunto de dados de forma a obter o tal $p < 5\%$ para demonstrar algo intencionalmente, o que fere os princípios básicos da inferência estatística e dá um "golpe mortal" na suposta infalibilidade do método científico, justamente no "calcanhar de Aquiles" da reprodutibilidade de um experimento. 

De toda forma vale lembrar que o valor-p é somente a ponta de um grande iceberg.

[^1]: Singh Chawla, D. Big names in statistics want to shake up much-maligned *P* value; *Nature* **548,** 16–17 (2017) doi:10.1038/nature.2017.22375.
